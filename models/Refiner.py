import torch.nn as nn
import torch
import torch.nn.functional as F


class Refiner(nn.Module):
    ## learn a kernel for each dot
    def __init__(self, kernel_size=64, downsample=1, softmax=False):
        super(Refiner, self).__init__()
        self.kernel_size = kernel_size
        self.softmax = softmax
        self.downsample = downsample # Some models return a downsampled density-map
        
        self.adapt = nn.Sequential(
                                   nn.Conv2d(3, 32, 3, 1, 1),
                                   nn.LeakyReLU(),
                                   nn.MaxPool2d(2), 
                                   nn.Conv2d(32, 64, 3, 1, 1),
                                   nn.LeakyReLU(),
                                   nn.MaxPool2d(2),
                                   nn.Conv2d(64, 128, 3, 1, 1),
                                   nn.LeakyReLU(),
                                   nn.MaxPool2d(2),
                                   nn.Conv2d(128, 128, 3, 1, 1),
                                   nn.LeakyReLU(),
                                   nn.MaxPool2d(2),
                                   nn.Conv2d(128, self.kernel_size**2, 3, 1, 1))

    def forward(self, batch_points, batch_img, shape):
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        kernels = self.adapt(batch_img)

        # Normalize the kernels using either softmax or min-max normalization
        # Softmax: Emphasizes larger values, outputs can be interpreted as probabilities
        # Non-softmax: Preserves relative differences, simple normalization
        # The choice affects the emphasis on stronger features and numerical properties
        if self.softmax:
            kernels = F.softmax(kernels, 1)
        else:
            kernels = kernels - torch.min(kernels, 1, True)[0]
            kernels = kernels / torch.sum(kernels, 1, True)

        # Add padding to the shape (for the density map) to accommodate kernel size
        padded_shape = (shape[0], shape[1], shape[2] + 2 * self.kernel_size, shape[3] + 2 * self.kernel_size)
        density = torch.zeros(padded_shape, device=device)

        # Generate density for each image in the batch
        for j, points in enumerate(batch_points):
            n = len(points)
            if n == 0:  # If there are no points 
                continue

            # For each object, place the kernel generated by the model
            for i in range(n):
                # Add the kernel_size so that it avoids the padding area
                x = int(points[i, 0] / self.downsample - self.kernel_size/2) + self.kernel_size
                y = int(points[i, 1] / self.downsample - self.kernel_size/2) + self.kernel_size
                xmax = x + self.kernel_size
                ymax = y + self.kernel_size

                # Conv and sum
                k = kernels[0, :, min(kernels.shape[2] - 1, int(points[i, 1] / 16)), min(kernels.shape[3] - 1, int(points[i, 0] / 16))].view(1, 1, self.kernel_size, self.kernel_size)
                density[j, :, x:xmax, y:ymax] += k[0]

        # Remove the padding and return the density map with the original shape
        density = density[:, :, self.kernel_size:-self.kernel_size, self.kernel_size:-self.kernel_size]
        return density

